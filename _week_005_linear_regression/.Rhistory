install.packages("LightGBM")
library(devtools)
install.packages("devtools")
options(devtools.install.args = "--no-multiarch")
install_github("Microsoft/LightGBM", subdir = "R-package")
install_github("Microsoft/LightGBM", subdir = "R-package")
options(devtools.install.args = "--no-multiarch")
install_github("Microsoft/LightGBM", subdir = "R-package")
# Create some variables
6+6
a = 3
b = 4
a
b
# Lets add them
a + b
# lets try a character vector
char = c("The brown cow", "said", "moo")
char[3]
# Boolean
bool = c(TRUE, TRUE, FALSE, T, F, T, F, F)
bool
# now lets create variables
v1 = c(1, 5, 9, 9, 5, 5, 5, 5)
v2 = c(a, b, 7, 5, 5, 5, 5, 5)
v1 * v2
# now lets put them in a matrix
m1 = matrix(c(v1, v2), nrow = length(v1))
m1
# rename the colums
colnames(m1) = c("First", "Second")
m1
rownames(m1) = 1:nrow(m1)
m1
# lets get a description of our matrix
summary(m1)
?summary
# lets get a compact description of our matrix
str(m1)
?str
# If we want to add a different type of variable to a matrix we need to use a data frame
df1 = data.frame(col1 = v1, col2 = v2, col4 = bool)
df1
str(df1)
# Today we look at Linear Regression in both R and TI.
# Please use the below book as a reference - its an  excellent resource for getting started in many ML tasks.
#    https://ibm.box.com/s/9qa5l146in39u2lmfb1ln29lfif8yr6v
# This removes anything from a previous session
rm(list=ls())
# Lets check current time
Sys.time()
# This removes anything from a previous session
rm(list=ls())
# Lets check current time
Sys.time()
# First we set our working directory
# Please change this to your working directory where the data is
# setwd("C:/Users/IBM_ADMIN/Box Sync/DScTraining/_2109_meeting")
setwd("/Users/dhanley2/Documents/ml_training_r/_005_meeting")
getwd()
# Libraries
library(ggplot2)
library(GGally)
library(Metrics)
library(caret)
# We are going to use the US cereals data set
cereals = read.csv("data/US_Cereals.csv")
str(cereals)
# Lets looks at these features visually
par(mfrow = c(4, 5))
for(i in 4:16) hist(cereals[,i], main = names(cereals)[i])
for(i in c(1:3, 17)) barplot(table(cereals[,i]), main = names(cereals)[i])
for(i in 4:16) hist(cereals[,i], main = names(cereals)[i])
# Name is just an ID, lets lake it out
# we also take out the categorial columns but we can try putting them back later
regCereals = cereals[,4:16]
# Lets start with Simple Linear Regression
?lm
lm.fit =lm(rating ~ sugars ,data= regCereals )
lm.fit
summary(lm.fit)
## NOTE : Explore the object lm.fit in your right top pane
lm.fit$coefficients
# Lets plot the line
par(mfrow = c(1, 1))
plot( regCereals$sugars, regCereals$rating, pch=19, xlab="Sugars", ylab = "Ratings" )
abline(lm.fit, col = "red")
# Now just to show how predict works, let predict the same points.
preds.lm = predict(lm.fit, regCereals)
points(regCereals$sugars, preds.lm, col="blue", pch=19, cex=1.2)
# Lets add jitter to the plot to see them better - this helps if you have overlapping points
plot( jitter(regCereals$sugars), jitter(regCereals$rating), pch=19, xlab="Sugars", ylab = "Ratings" )
abline(lm.fit, col = "red")
points(jitter(regCereals$sugars), jitter(preds.lm), col="blue")
# what does a non linear model look like - adding non linear terms
lm.fit2 =lm(rating ~ regCereals$sugars + I(sugars^2) , data = regCereals )
preds.lm2 = as.numeric(predict(lm.fit2, regCereals))
plot( regCereals$sugars, regCereals$rating, pch=19, xlab="Sugars", ylab = "Ratings" )
points(regCereals$sugars,preds.lm2 ,col="red", pch=19, cex = 2)
# Now lets look at multiple linear regression
# There are two ways of doing this but they both gove the same result
lmm.fit.1 =lm(rating ~ calories + protein + fat + sodium + fiber + carbo + sugars + potass + vitamins + shelf + weight + cups, data = regCereals )
lmm.fit =lm(rating ~ . ,data = regCereals )
all.equal(lmm.fit.1$coefficients, lmm.fit$coefficients)
# Not lets look at the model and make a perdiction
summary(lmm.fit)
plot(lmm.fit)
sqrt(mean((regCereals$rating - preds.lm)^2)) # [1] 9.075487
rmse(regCereals$rating, preds.lm) # [1] 9.075487
rmse(regCereals$rating, preds.lm2) # [1] 8.730459
rmse(regCereals$rating, preds.lmm) # [1] 2.775257e-07 = 0.0000002775257
# this is an amazing improvement
#       A bit too good to be true.
#             this is most likely because we trained and tested on the same data.
########################################################################################
########    Test and train partition is very important for testing models     ##########
########################################################################################
# Now lets try making a test partition and training and predicting again.
# Caret is very good for straified sampling of data
idx = createFolds(regCereals$rating, k = 3, list = FALSE)
head(idx, 10)
head(regCereals$rating, 10)
par(mfrow = c(2, 2))
for(i in 1:3) hist(regCereals$rating[idx==i], main=paste0("Fold: ", i), xlim = c(0,100), ylim=c(0,10))
X_train = regCereals[idx!=1,]              # Predictors for training
X_test  = regCereals[idx==1,]              # Predictors for testing
y_train = regCereals$rating[idx!=1]        # Response for training
y_test  = regCereals$rating[idx==1]        # Response for testing
X_train$rating = NULL                      # Remove reponse from Predictors data set
X_test $rating = NULL
lm.fit  =lm(y_train ~ sugars , data= X_train )
lmm.fit =lm(y_train ~ .      , data= X_train )
preds.lm  = predict(lm.fit, X_test)
preds.lmm = predict(lmm.fit, X_test)
rmse(y_test, preds.lm) # [1] 8.495122
rmse(y_test, preds.lmm) # [1] 3.355702e-07 = 0.000000335570
# I'm amazed that the error is so low. I can only conclude one of two reaons,
#    1) I have a bug in the script which is allowing lm overfit
#    2) The rating was derived originally from a linear function.
# You can read up more on adding interaction terms or polynomials in linear terms on page 115 forward of the book.
#    https://ibm.box.com/s/9qa5l146in39u2lmfb1ln29lfif8yr6v
# Let say we wanted to bring back our categorical data.
# How can we represent categorical data in a linear model. One common way is to "One hot encode".
?stats::model.matrix
str(cereals)
par(mfrow = c(1, 1))
plot(table(cereals$mfr))
mfr_ohe = model.matrix(~ cereals$mfr + 0)
# lets looks at the OHE table and compare it to the categorical values
head(mfr_ohe)
head(cereals$mfr)
# Now we have represented a categorical in numerical format
regCereals = cbind(regCereals, mfr_ohe)
str(regCereals)
lmm1.fit =lm(rating ~ . ,data = regCereals )
preds1.lmm = predict(lmm1.fit, regCereals)
rmse(regCereals$rating, preds1.lmm)  # [1] 2.661632e-07    ... before was [1] 2.775257e-07
library(Metrics)
install.packages("Metrics")
?rmse
# Today we look at Linear Regression in both R and TI.
# Please use the below book as a reference - its an  excellent resource for getting started in many ML tasks.
#    https://ibm.box.com/s/9qa5l146in39u2lmfb1ln29lfif8yr6v
# This removes anything from a previous session
rm(list=ls())
# Lets check current time
Sys.time()
# First we set our working directory
# Please change this to your working directory where the data is
# setwd("C:/Users/IBM_ADMIN/Box Sync/DScTraining/_2109_meeting")
setwd("/Users/dhanley2/Documents/ml_training_r/_005_meeting")
getwd()
# Libraries
library(ggplot2)
library(GGally)
library(Metrics)
library(caret)
# We are going to use the US cereals data set
cereals = read.csv("data/US_Cereals.csv")
str(cereals)
# Lets looks at these features visually
par(mfrow = c(4, 5))
for(i in 4:16) hist(cereals[,i], main = names(cereals)[i])
for(i in c(1:3, 17)) barplot(table(cereals[,i]), main = names(cereals)[i])
# Name is just an ID, lets lake it out
# we also take out the categorial columns but we can try putting them back later
regCereals = cereals[,4:16]
# Lets start with Simple Linear Regression
?lm
lm.fit =lm(rating ~ sugars ,data= regCereals )
lm.fit
summary(lm.fit)
## NOTE : Explore the object lm.fit in your right top pane
lm.fit$coefficients
# Lets plot the line
par(mfrow = c(1, 1))
plot( regCereals$sugars, regCereals$rating, pch=19, xlab="Sugars", ylab = "Ratings" )
abline(lm.fit, col = "red")
# Now just to show how predict works, let predict the same points.
preds.lm = predict(lm.fit, regCereals)
points(regCereals$sugars, preds.lm, col="blue", pch=19, cex=1.2)
# Lets add jitter to the plot to see them better - this helps if you have overlapping points
plot( jitter(regCereals$sugars), jitter(regCereals$rating), pch=19, xlab="Sugars", ylab = "Ratings" )
abline(lm.fit, col = "red")
points(jitter(regCereals$sugars), jitter(preds.lm), col="blue")
# what does a non linear model look like - adding non linear terms
lm.fit2 =lm(rating ~ regCereals$sugars + I(sugars^2) , data = regCereals )
preds.lm2 = as.numeric(predict(lm.fit2, regCereals))
plot( regCereals$sugars, regCereals$rating, pch=19, xlab="Sugars", ylab = "Ratings" )
points(regCereals$sugars,preds.lm2 ,col="red", pch=19, cex = 2)
# Now lets look at multiple linear regression
# There are two ways of doing this but they both gove the same result
lmm.fit.1 =lm(rating ~ calories + protein + fat + sodium + fiber + carbo + sugars + potass + vitamins + shelf + weight + cups, data = regCereals )
lmm.fit =lm(rating ~ . ,data = regCereals )
all.equal(lmm.fit.1$coefficients, lmm.fit$coefficients)
# Not lets look at the model and make a perdiction
summary(lmm.fit)
plot(lmm.fit)
sqrt(mean((regCereals$rating - preds.lm)^2)) # [1] 9.075487
rmse(regCereals$rating, preds.lm) # [1] 9.075487
rmse(regCereals$rating, preds.lm2) # [1] 8.730459
rmse(regCereals$rating, preds.lmm) # [1] 2.775257e-07 = 0.0000002775257
# this is an amazing improvement
#       A bit too good to be true.
#             this is most likely because we trained and tested on the same data.
########################################################################################
########    Test and train partition is very important for testing models     ##########
########################################################################################
# Now lets try making a test partition and training and predicting again.
# Caret is very good for straified sampling of data
idx = createFolds(regCereals$rating, k = 3, list = FALSE)
head(idx, 10)
head(regCereals$rating, 10)
par(mfrow = c(2, 2))
for(i in 1:3) hist(regCereals$rating[idx==i], main=paste0("Fold: ", i), xlim = c(0,100), ylim=c(0,10))
X_train = regCereals[idx!=1,]              # Predictors for training
X_test  = regCereals[idx==1,]              # Predictors for testing
y_train = regCereals$rating[idx!=1]        # Response for training
y_test  = regCereals$rating[idx==1]        # Response for testing
X_train$rating = NULL                      # Remove reponse from Predictors data set
X_test $rating = NULL
lm.fit  =lm(y_train ~ sugars , data= X_train )
lmm.fit =lm(y_train ~ .      , data= X_train )
preds.lm  = predict(lm.fit, X_test)
preds.lmm = predict(lmm.fit, X_test)
rmse(y_test, preds.lm) # [1] 8.495122
rmse(y_test, preds.lmm) # [1] 3.355702e-07 = 0.000000335570
# I'm amazed that the error is so low. I can only conclude one of two reaons,
#    1) I have a bug in the script which is allowing lm overfit
#    2) The rating was derived originally from a linear function.
# You can read up more on adding interaction terms or polynomials in linear terms on page 115 forward of the book.
#    https://ibm.box.com/s/9qa5l146in39u2lmfb1ln29lfif8yr6v
# Let say we wanted to bring back our categorical data.
# How can we represent categorical data in a linear model. One common way is to "One hot encode".
?stats::model.matrix
str(cereals)
par(mfrow = c(1, 1))
plot(table(cereals$mfr))
mfr_ohe = model.matrix(~ cereals$mfr + 0)
# lets looks at the OHE table and compare it to the categorical values
head(mfr_ohe)
head(cereals$mfr)
# Now we have represented a categorical in numerical format
regCereals = cbind(regCereals, mfr_ohe)
str(regCereals)
lmm1.fit =lm(rating ~ . ,data = regCereals )
preds1.lmm = predict(lmm1.fit, regCereals)
rmse(regCereals$rating, preds1.lmm)  # [1] 2.661632e-07    ... before was [1] 2.775257e-07
?lm
# Today we look at Linear Regression in both R and TI.
# Please use the below book as a reference - its an  excellent resource for getting started in many ML tasks.
#    https://ibm.box.com/s/9qa5l146in39u2lmfb1ln29lfif8yr6v
# This removes anything from a previous session
rm(list=ls())
# Lets check current time
Sys.time()
# First we set our working directory
# Please change this to your working directory where the data is
# setwd("C:/Users/IBM_ADMIN/Box Sync/DScTraining/_2109_meeting")
setwd("/Users/dhanley2/Documents/ml_training_r/_005_meeting")
getwd()
# Libraries
library(ggplot2)
library(GGally)
library(ggplot2)
library(GGally)
library(Metrics)
library(caret)
install.packages("GGally")
library(ggplot2)
library(GGally)
library(Metrics)
library(caret)
# We are going to use the US cereals data set
cereals = read.csv("data/US_Cereals.csv")
library(Metrics)
str(cereals)
# Lets looks at these features visually
par(mfrow = c(4, 5))
for(i in 4:16) hist(cereals[,i], main = names(cereals)[i])
for(i in c(1:3, 17)) barplot(table(cereals[,i]), main = names(cereals)[i])
par(mfrow = c(4, 5))
for(i in 4:16) hist(cereals[,i], main = names(cereals)[i], col = "red")
regCereals = cereals[,4:16]
?lm
str(regCereals)
lm.fit =lm(rating ~ sugars ,data= regCereals )
lm.fit
summary(lm.fit)
lm.fit$coefficients
lm.fit$model$sugars
lm.fit$df.residual
# Lets plot the line
par(mfrow = c(1, 1))
plot( regCereals$sugars, regCereals$rating, pch=19, xlab="Sugars", ylab = "Ratings" )
abline(lm.fit, col = "red")
abline(lm.fit, col = "red")
abline(h=60, col="blue")
abline(v=60, col="blue")
abline(h=50, col="blue", type="-")
preds.lm = predict(lm.fit, regCereals)
points(regCereals$sugars, preds.lm, col="blue", pch=19, cex=1.2)
plot( jitter(regCereals$sugars), jitter(regCereals$rating), pch=19, xlab="Sugars", ylab = "Ratings" )
abline(lm.fit, col = "red")
points(jitter(regCereals$sugars), jitter(preds.lm), col="blue")
# what does a non linear model look like - adding non linear terms
lm.fit2 =lm(rating ~ regCereals$sugars + I(sugars^2) , data = regCereals )
preds.lm2 = as.numeric(predict(lm.fit2, regCereals))
plot( regCereals$sugars, regCereals$rating, pch=19, xlab="Sugars", ylab = "Ratings" )
points(regCereals$sugars,preds.lm2 ,col="red", pch=19, cex = 2)
lmm.fit.1 =lm(rating ~ calories + protein + fat + sodium + fiber + carbo + sugars + potass + vitamins + shelf + weight + cups, data = regCereals )
lmm.fit =lm(rating ~ . ,data = regCereals )
all.equal(lmm.fit.1$coefficients, lmm.fit$coefficients)
lmm.fit$coefficients
# Not lets look at the model and make a perdiction
summary(lmm.fit)
plot(lmm.fit)
# Lets make a prediction using both models and see which performs the best.
preds.lmm = predict(lmm.fit, regCereals)
?rmse
regCereals$rating
preds.lm
(regCereals$rating - preds.lm)
(regCereals$rating - preds.lm)^2
mean((regCereals$rating - preds.lm)^2)
sqrt(mean((regCereals$rating - preds.lm)^2))
mean((regCereals$rating - preds.lm))
mean(abs(regCereals$rating - preds.lm))
sqrt(mean((regCereals$rating - preds.lm)^2)) # [1] 9.075487
rmse(regCereals$rating, preds.lm) # [1] 9.075487
rmse(regCereals$rating, preds.lm2) # [1] 8.730459
rmse(regCereals$rating, preds.lmm) # [1] 2.775257e-07 = 0.0000002775257
regCereals$rating
idx = createFolds(regCereals$rating, k = 3, list = FALSE)
idx = createFolds(regCereals$rating, k = 3, list = FALSE)
head(idx, 10)
head(regCereals$rating, 10)
par(mfrow = c(2, 2))
i=2
idx==i
for(i in 1:3) hist(regCereals$rating[idx==i], main=paste0("Fold: ", i), xlim = c(0,100), ylim=c(0,10))
X_train = regCereals[idx!=1,]              # Predictors for training
X_test  = regCereals[idx==1,]              # Predictors for testing
y_train = regCereals$rating[idx!=1]        # Response for training
y_test  = regCereals$rating[idx==1]        # Response for testing
X_train$rating = NULL                      # Remove reponse from Predictors data set
X_test $rating = NULL
lm.fit  =lm(y_train ~ sugars , data= X_train )
lmm.fit =lm(y_train ~ .      , data= X_train )
preds.lm  = predict(lm.fit, X_test)
preds.lmm = predict(lmm.fit, X_test)
rmse(y_test, preds.lm) # [1] 8.495122
rmse(y_test, preds.lmm) # [1] 3.355702e-07 = 0.000000335570
