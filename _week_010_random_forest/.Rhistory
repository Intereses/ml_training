fold1 = fold2 = vec
library(zoo)
library(caret)
set.seed(1)
vec = rep(0, 40)
vec[sample(1:40, 10)] = 1
# look at vector
vec
# window size of 5
rollmean(vec, 5, fill=NA)
# Create an index
idx = createFolds(vec, 2, list=F)
idx
# Rollmean folds
fold1 = fold2 = vec
# get OOF rollmean of fold1
fold1[idx==2] = 0
fold1 = rollmean(fold1, 5, fill=NA)
# get OOF rollmean of fold2
fold1[idx==1] = 0
fold1 = rollmean(fold1, 5, fill=NA)
library(zoo)
library(caret)
set.seed(1)
vec = rep(0, 40)
vec[sample(1:40, 10)] = 1
# look at vector
vec
# window size of 5
rollmean(vec, 5, fill=NA)
# Create an index
idx = createFolds(vec, 2, list=F)
idx
# Rollmean folds
fold1 = fold2 = vec
# get OOF rollmean of fold1
fold1[idx==2] = 0
fold1 = rollmean(fold1, 5, fill=NA)
# get OOF rollmean of fold2
fold2[idx==1] = 0
fold2 = rollmean(fold2, 5, fill=NA)
result = fold1
# Get the result using only OOF elements
result = fold1
result[idx==1] = fold2
result[idx==1] = fold2[idx==1]
result = fold1
result[idx==1] = fold2[idx==1]
result
library(zoo);library(caret)
library(zoo);library(caret);set.seed(1)
vec = rep(0, 40)
vec[sample(1:40, 10)] = 1
# look at vector
vec
# window size of 5
rollmean(vec, 5, fill=NA)
# Create an index
idx = createFolds(vec, 2, list=F)
idx
# Rollmean folds
fold1 = fold2 = vec
# get OOF rollmean of fold1
fold1[idx==2] = 0
fold1 = rollmean(fold1, 5, fill=NA)
# get OOF rollmean of fold2
fold2[idx==1] = 0
fold2 = rollmean(fold2, 5, fill=NA)
# Get the result using only OOF elements
result = fold1
result[idx==1] = fold2[idx==1]
result
library(zoo);library(caret);set.seed(1)
vec = rep(0, 40)
vec[sample(1:40, 10)] = 1
# Create a vector will use as our target
vec
# for example, lets look at the in-fold rollmean with window size of 5
rollmean(vec, 5, fill=NA)
# Lets get the out-of-fold rollmean with window of 5
# Create an index based on the vector with two folds
idx = createFolds(vec, 2, list=F)
idx
# Rollmean folds
fold1 = fold2 = vec
# get OOF rollmean of fold1
fold1[idx==2] = 0
fold1 = rollmean(fold1, 5, fill=NA)
# get OOF rollmean of fold2
fold2[idx==1] = 0
fold2 = rollmean(fold2, 5, fill=NA)
# Get the result using only OOF elements
result = fold1
result[idx==1] = fold2[idx==1]
result
library(zoo);library(caret);set.seed(1)
vec = rep(0, 40)
vec[sample(1:40, 10)] = 1
# Create a vector will use as our target
vec
# The next line is the infold rollmean, with window of 5. this will overfit on your training data
rollmean(vec, 5, fill=NA)
# Now, lets get the out-of-fold rollmean with window of 5
# We need an index based on the vector with two folds
idx = createFolds(vec, 2, list=F)
idx
# Create a vector of results for each fold.
fold1 = fold2 = vec
# get OOF rollmean of fold1
fold1[idx==2] = 0
fold1 = rollmean(fold1, 5, fill=NA)
# get OOF rollmean of fold2
fold2[idx==1] = 0
fold2 = rollmean(fold2, 5, fill=NA)
# Get the result using only OOF elements
result = fold1
result[idx==1] = fold2[idx==1]
result
# This removes anything from a previous session
rm(list=ls())
# Lets check current time
Sys.time()
# First we set our working directory
# Please change this to your working directory where the data is
setwd("C:/Users/IBM_ADMIN/Box Sync/DScTraining/_0911_meeting")
getwd()
# Libraries
library(tree)
library(MASS)
library(Metrics)
library(caret)
library (randomForest)
set.seed(1)
# Lets look at Hitters data
data("Boston")
str(Boston)
# Lets create our training and test data
# Median house value is the target
x=Boston
x$medv = NULL
y=Boston$medv
# Lets split our data into two folds and predict
set.seed(1)
ind = createFolds(y, 2, list=F)
X_train = x[ind==1, ]
X_test  = x[ind!=1, ]
y_train = y[ind==1 ]
y_test  = y[ind!=1 ]
# Now lets model
tree.boston = tree(y_train ~ . , X_train)
summary (tree.boston )
plot(tree.boston)
text(tree.boston ,pretty =0)
?tree
cv.boston =cv.tree(tree.boston )
plot(cv.boston$size ,cv.boston$dev ,type="b")
# Now we prune the tree
prune.boston =prune.tree(tree.boston ,best =7)
plot(prune.boston )
text(prune.boston ,pretty =0)
# Let us predict on the tree
ypred.tree = predict(tree.boston , newdata = X_test)
plot(y_test, ypred.tree, col="blue", pch=19)
abline (0,1)
rmse(y_test, ypred.tree)
# 5.059668
# Lets compare to a linear model type approach
lm.mod = lm(y_train~., data=data.frame(X_train))
ypred.lm = predict(lm.mod, data.frame(X_test))
points(ypred.lm, col="red", pch=19)
rmse(y_test, ypred.lm)
# 4.679406
# Lets check if we combine the two
# These two models work a lot differently, so a combination of the two can often do better than one on its own.
rmse(y_test, (ypred.lm+ypred.tree)/2)
# [1] 4.22
# Random Forest
?randomForest
set.seed (1)
rf.mod = randomForest(y_train~.,data=X_train, mtry=6, importance =TRUE) # ranger
rf.mod = randomForest(as.factor(loan_status)~.,data=X_train, mtry=6, importance =TRUE) # ranger
ypred.rf = predict(rf.mod ,newdata = X_test)
rmse(y_test, ypred.rf)
# [1] 3.907974
# Now lets look at the importance
importance(rf.mod)
# Lets sort it
imp = importance(rf.mod)
imp[order(imp[,1], decreasing = T),]
# Now let's tune to see if we can do better
# First we tune over a different number of variables per tree
set.seed(7)
mtry = 2:13
error = rep(0, length(2:13))
for(m in 1:length(mtry)){
rf.mod = randomForest(y_train~.,data=X_train, mtry=mtry[m], ntree = 500)
ypred.rf = predict(rf.mod ,newdata = X_test)
error[m] = rmse(y_test, ypred.rf)
plot(mtry, error, pch = 19, col="blue", ylim = c(3.5, 4.5), main = "Random Forest : Error for Number of Random Features per Tree")
}
# Now we keep number of variables constant and try changing the number of trees
m = 5
trees = seq(100, 3000, 100)
error = rep(0, length(trees))
for(t in 1:length(error)){
rf.mod = randomForest(y_train~.,data=X_train, mtry=m, ntree = trees[t])
ypred.rf = predict(rf.mod ,newdata = X_test)
error[t] = rmse(y_test, ypred.rf)
plot(trees, error, pch = 19, col="blue", ylim = c(3.5, 4.5), main = "Random Forest : Error for Different Number of Trees")
}
# This removes anything from a previous session
rm(list=ls())
# Lets check current time
Sys.time()
# First we set our working directory
# Please change this to your working directory where the data is
setwd("C:/Users/IBM_ADMIN/Box Sync/DScTraining/_0911_meeting")
getwd()
# Libraries
library(tree)
library(MASS)
library(Metrics)
library(caret)
library (randomForest)
set.seed(1)
# Lets look at Hitters data
data("Boston")
str(Boston)
# Lets create our training and test data
# Median house value is the target
x=Boston
x$medv = NULL
y=Boston$medv
# Lets split our data into two folds and predict
set.seed(1)
ind = createFolds(y, 2, list=F)
X_train = x[ind==1, ]
X_test  = x[ind!=1, ]
y_train = y[ind==1 ]
y_test  = y[ind!=1 ]
# Now lets model
tree.boston = tree(y_train ~ . , X_train)
summary (tree.boston )
plot(tree.boston)
text(tree.boston ,pretty =0)
?tree
cv.boston =cv.tree(tree.boston )
plot(cv.boston$size ,cv.boston$dev ,type="b")
# Now we prune the tree
prune.boston =prune.tree(tree.boston ,best =7)
plot(prune.boston )
text(prune.boston ,pretty =0)
# Let us predict on the tree
ypred.tree = predict(tree.boston , newdata = X_test)
plot(y_test, ypred.tree, col="blue", pch=19)
abline (0,1)
rmse(y_test, ypred.tree)
# 5.059668
# Lets compare to a linear model type approach
lm.mod = lm(y_train~., data=data.frame(X_train))
ypred.lm = predict(lm.mod, data.frame(X_test))
points(ypred.lm, col="red", pch=19)
rmse(y_test, ypred.lm)
# 4.679406
# Lets check if we combine the two
# These two models work a lot differently, so a combination of the two can often do better than one on its own.
rmse(y_test, (ypred.lm+ypred.tree)/2)
# [1] 4.22
# Random Forest
?randomForest
set.seed (1)
rf.mod = randomForest(y_train~.,data=X_train, mtry=6, importance =TRUE) # ranger
# rf.mod = randomForest(as.factor(loan_status)~.,data=X_train, mtry=6, importance =TRUE) # ranger
ypred.rf = predict(rf.mod ,newdata = X_test)
rmse(y_test, ypred.rf)
# [1] 3.907974
# Finally, lets check blending the randomforest and linear model.
rmse(y_test, (ypred.rf+ypred.lm)/2)
# [1] 3.951797
# Lets try a weighted blend
rmse(y_test, ((ypred.rf*0.8)+(ypred.lm*0.2)))
# [1] 3.793317
# .... A little better, but hard to tell... would be better to run with different seeds to verify.
library(xgboost)
?xgb.train
class(X_train)
dtrain <- xgb.DMatrix(data = as.matrix(X_train), label = y)
dtrain <- xgb.DMatrix(data = as.matrix(X_train), label = y_train)
?xgb.train
dtrain <- xgb.DMatrix(data = as.matrix(X_train), label = y_train)
xg.mod <- xgboost(data = dtrain,
max.depth = 2,
eta = 0.1,
nthread = 8,
nround = 100,
objective = "reg:linear",
verbose = 0)
dtrain <- xgb.DMatrix(data = as.matrix(X_train), label = y_train)
xg.mod <- xgboost(data = dtrain,
max.depth = 2,
eta = 0.1,
nthread = 8,
nround = 100,
objective = "reg:linear",
verbose = 2)
xg.mod <- xgboost(data = dtrain,
max.depth = 2,
eta = 0.1,
nthread = 8,
nround = 100,
objective = "reg:linear",
verbose = 1)
class(dtrain)
# For xgb we need to set the data in a special format
dtrain <- xgb.DMatrix(data = as.matrix(X_train), label = y_train)
# Lets train a model and output the training error as we go
xg.mod <- xgboost(data = dtrain,
max.depth = 2,  # depth of all the trees
eta = 0.1,      # Learning rate of trees
nthread = 8,    # Number of threads or parallel processes
nround = 100,   # number of
objective = "reg:linear",
print.every.n = 10,
verbose = 1)
# Xgb also lets us to cross validate
xg.mod <- xgb.cv(data = dtrain,
max.depth = 2,                   # depth of all the trees
eta = 0.1,                       # Learning rate of trees
nthread = 8,                     # Number of threads or parallel processes
nround = 100,                    # number of trees to create
nfold = 5,
objective = "reg:linear",        # We want to optimise on a regression porblem
print.every.n = 10,
verbose = 1)
# Xgb also lets us to cross validate
xg.mod <- xgb.cv(data = dtrain,
max.depth = 2,                   # depth of all the trees
eta = 0.1,                       # Learning rate of trees
nthread = 8,                     # Number of threads or parallel processes
nround = 1000,                    # number of trees to create
nfold = 5,
objective = "reg:linear",        # We want to optimise on a regression porblem
print.every.n = 10,
verbose = 1)
# Xgb also lets us to cross validate
xg.mod <- xgb.cv(data = dtrain,
max.depth = 2,                   # depth of all the trees
eta = 0.01,                       # Learning rate of trees
nthread = 8,                     # Number of threads or parallel processes
nround = 500,                    # number of trees to create
nfold = 5,
objective = "reg:linear",        # We want to optimise on a regression porblem
print.every.n = 10,
verbose = 1)
# Xgb also lets us to cross validate
xg.mod <- xgb.cv(data = dtrain,
max.depth = 2,                   # depth of all the trees
eta = 0.01,                      # Learning rate of trees
nthread = 8,                     # Number of threads or parallel processes
nround = 2000,                   # number of trees to create
nfold = 5,
objective = "reg:linear",        # We want to optimise on a regression porblem
print.every.n = 10,
verbose = 1)
# Xgb also lets us to cross validate
xg.mod <- xgb.cv(data = dtrain,
max.depth = 2,                   # depth of all the trees
eta = 0.01,                      # Learning rate of trees
nthread = 8,                     # Number of threads or parallel processes
nround = 2000,                   # number of trees to create
nfold = 5,
objective = "reg:linear",        # We want to optimise on a regression porblem
print.every.n = 50,
verbose = 1)
# Xgb also lets us to cross validate
xg.mod <- xgb.cv(data = dtrain,
max.depth = 2,                   # depth of all the trees
eta = 0.01,                      # Learning rate of trees
nthread = 8,                     # Number of threads or parallel processes
nround = 1500,                   # number of trees to create
nfold = 5,
objective = "reg:linear",        # We want to optimise on a regression porblem
print.every.n = 50,
verbose = 1)
# Xgb also lets us to cross validate
xg.mod <- xgb.cv(data = dtrain,
max.depth = 3,                   # depth of all the trees
eta = 0.01,                      # Learning rate of trees
nthread = 8,                     # Number of threads or parallel processes
nround = 1500,                   # number of trees to create
nfold = 5,
objective = "reg:linear",        # We want to optimise on a regression porblem
print.every.n = 50,
verbose = 1)
# Xgb also lets us to cross validate
xg.mod <- xgb.cv(data = dtrain,
max.depth = 3,                   # depth of all the trees
eta = 0.01,                      # Learning rate of trees
nthread = 8,                     # Number of threads or parallel processes
nround = 2000,                   # number of trees to create
nfold = 5,
objective = "reg:linear",        # We want to optimise on a regression porblem
print.every.n = 50,
verbose = 1)
# Xgb also lets us to cross validate
xg.mod <- xgb.cv(data = dtrain,
max.depth = 2,                   # depth of all the trees
eta = 0.01,                      # Learning rate of trees
nthread = 8,                     # Number of threads or parallel processes
nround = 2000,                   # number of trees to create
nfold = 5,
objective = "reg:linear",        # We want to optimise on a regression porblem
print.every.n = 50,              # We only print teh
verbose = 1)
# We can ask xgb to stop it when it gets to the best round
xg.mod <- xgb.cv(data = dtrain,
max.depth = 2,                   # depth of all the trees
eta = 0.01,                      # Learning rate of trees
nthread = 8,                     # Number of threads or parallel processes
nround = 2000,                   # number of trees to create
nfold = 5,                       # Use 5 fold cross validate
early.stop.round = 50,
objective = "reg:linear",        # We want to optimise on a regression porblem
print.every.n = 50,              # We only print teh
verbose = 1)
# We can ask xgb to stop it when it gets to the best round
xg.mod <- xgb.cv(data = dtrain,
max.depth = 2,                   # depth of all the trees
eta = 0.01,                      # Learning rate of trees
nthread = 8,                     # Number of threads or parallel processes
nround = 2000,                   # number of trees to create
nfold = 5,                       # Use 5 fold cross validate
early.stop.round = 50,           # stop it when it does not improve after 50 trees
maximize = F,                    # linked to watchlist; definition of improve is to minimise
objective = "reg:linear",        # We want to optimise on a regression porblem
print.every.n = 50,              # We only print teh
verbose = 1)
# So now lets train our data using the best number of trees
xg.mod <- xgboost(data = dtrain,
max.depth = 2,                   # depth of all the trees
eta = 0.01,                      # Learning rate of trees
nthread = 8,                     # Number of threads or parallel processes
nround = 1154,                   # number of trees to create
objective = "reg:linear",        # We want to optimise on a regression porblem
print.every.n = 10,
verbose = 0)
xg.mod
names <- colnames(X_train)
# Compute feature importance matrix
importance_matrix <- xgb.importance(names, model = bst)
importance_matrix <- xgb.importance(names, model = xg.mod)
# Nice graph
xgb.plot.importance(importance_matrix[1:10,])
y_pred.xg = predict(xg.mod, as.matrix(X_test))
rmse(y_test, ypred.xg)
rmse(y_test, y_pred.xg)
rmse(y_test, ypred.lm)
# 4.679406
rmse(y_test, ypred.rf)
# [1] 3.907974
matrix(c(1,1,0,0,0,0,0,0,1), ncol = 3)
m = matrix(c(1,1,0,0,0,0,0,0,1), ncol = 3)
plot(vec, type = "l", col="blue")
library(zoo);library(caret);set.seed(1)
vec = rep(0, 40)
vec[sample(1:40, 10)] = 1
# Create a vector will use as our target
vec
# The next line is the infold rollmean, with window of 5. this will overfit on your training data
rollmean(vec, 5, fill=NA)
# Now, lets get the out-of-fold rollmean with window of 5
# We need an index based on the vector with two folds
idx = createFolds(vec, 2, list=F)
idx
# Create a vector of results for each fold.
fold1 = fold2 = vec
# get OOF rollmean of fold1
fold1[idx==2] = 0
fold1 = rollmean(fold1, 5, fill=NA)
# get OOF rollmean of fold2
fold2[idx==1] = 0
fold2 = rollmean(fold2, 5, fill=NA)
# Get the result using only OOF elements
result = fold1
result[idx==1] = fold2[idx==1]
result
plot(vec, type = "l", col="blue")
lines(rollmean(vec, 5, fill=NA), type = "l", col="blue")
plot(vec, type = "l", col="blue")
lines(rollmean(vec, 5, fill=NA), type = "l", col="red")
lines(rollmean(result, 5, fill=NA), type = "l", col="green")
plot(vec, type = "l", col="blue")
lines(rollmean(vec, 5, fill=NA), type = "l", col="red")
lines(result, type = "l", col="green")
plot(vec, type = "l", col="blue", lty = 2)
lines(rollmean(vec, 5, fill=NA), type = "l", col="red", lty = 2)
lines(result, type = "l", col="green", lty = 2)
text("topleft",col=c("Target", "rollmean", "Out of fold rollmeant")), lty = 2)
text("topleft",col=c("Target", "rollmean", "Out of fold rollmeant"), lty = 2)
legend("topleft",legend=c("Target", "rollmean", "Out of fold rollmeant"), lty = 2)
plot(vec, type = "l", col="blue", lty = 2)
lines(rollmean(vec, 5, fill=NA), type = "l", col="red", lty = 2)
lines(result, type = "l", col="green", lty = 2)
legend("topleft",legend=c("Target", "rollmean", "Out of fold rollmeant"), col=c("blue", "red", "green"), lty = 2)
plot(vec, type = "l", col="blue", lty = 2, lwd=3)
plot(vec, type = "l", col="blue", lty = 2, lwd=3)
lines(rollmean(vec, 5, fill=NA), type = "l", col="red", lty = 2, lwd=3)
lines(result, type = "l", col="green", lty = 2, lwd=3)
plot(vec, type = "l", col="blue", lty = 2, lwd=3)
lines(rollmean(vec, 5, fill=NA), type = "l", col="red", lty = 2, lwd=3)
lines(result, type = "l", col="green", lty = 2, lwd=3)
legend("topleft",legend=c("Target", "rollmean", "Out of fold rollmeant"), col=c("blue", "red", "green"), lty = 2, lwd=3)
