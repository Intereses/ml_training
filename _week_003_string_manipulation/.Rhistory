install.packages("LightGBM")
library(devtools)
install.packages("devtools")
options(devtools.install.args = "--no-multiarch")
install_github("Microsoft/LightGBM", subdir = "R-package")
install_github("Microsoft/LightGBM", subdir = "R-package")
options(devtools.install.args = "--no-multiarch")
install_github("Microsoft/LightGBM", subdir = "R-package")
# Create some variables
6+6
a = 3
b = 4
a
b
# Lets add them
a + b
# lets try a character vector
char = c("The brown cow", "said", "moo")
char[3]
# Boolean
bool = c(TRUE, TRUE, FALSE, T, F, T, F, F)
bool
# now lets create variables
v1 = c(1, 5, 9, 9, 5, 5, 5, 5)
v2 = c(a, b, 7, 5, 5, 5, 5, 5)
v1 * v2
# now lets put them in a matrix
m1 = matrix(c(v1, v2), nrow = length(v1))
m1
# rename the colums
colnames(m1) = c("First", "Second")
m1
rownames(m1) = 1:nrow(m1)
m1
# lets get a description of our matrix
summary(m1)
?summary
# lets get a compact description of our matrix
str(m1)
?str
# If we want to add a different type of variable to a matrix we need to use a data frame
df1 = data.frame(col1 = v1, col2 = v2, col4 = bool)
df1
str(df1)
# Today we look at string operations in R.
# You will need to load the library stringr.
# In TI data preprocessing, we often want to,
#     Abbreviate strings (such as reducing the length of survey questions)
#     Stem strings - eg. we may have multiple job types which are close (System Admin & system Administrator) and we want to make them the same
#     Find or subset rows containing a certain string
#     Giving names to oridinal categories
# http://gastonsanchez.com/Handling_and_Processing_Strings_in_R.pdf
# http://www2.rdatamining.com/uploads/5/7/1/3/57136767/rdatamining-slides-text-mining.pdf
# This removes anything from a previous session
rm(list=ls())
# Lets check current time
Sys.time()
# First we set our working directory
# Please change this to your working directory where the data is
setwd("C:/Users/IBM_ADMIN/Box Sync/DScTraining/_3108_meeting")
getwd()
# Lets load our data set, and take a quick look at it
data("USArrests")
head(USArrests)
dim(USArrests)
# We see the rows are called after states of the US, and the column is the number of arrests per crime
states = rownames(USArrests)
states
substr(states, 1, 4)
# We can also use the abbreviate function which reduces to unique characters
states2 = abbreviate(states)
states2
abbreviate(states, minlength = 5)
# Lets look at the longest state name
states[1]
nchar(states[1:10])
states[which(nchar(states) > max(nchar(states))-5)]
# Get states with the string "w"
grep("w", states, value = T)
# Notice we miss Washington
grep("[wW]", states, value = T)  # ... this searches for both lower and upper case
# Alternate approach is make states lower case
tolower(states)
grep("w", tolower(states), value = T)
# We can also do uppercase
toupper(states)
# Or we can check in the help
?grep
grep("w", states, ignore.case = T, value = T)
# Now lets say we want to look for specific letters in a strings
library(stringr)
states[1]
str_count(states[1], "a")
states[7]
str_count(states[7], "a")
str_count(states, "a")
# Now we will look at String Manipulations
pi
PI = paste("Proj/Act", pi)
PI
IloveR = paste("I", "love", "R", sep = "-")
IloveR
paste("Column", 1:5, sep = ".")
# Create dynamic columns
letters
LETTERS
paste(LETTERS, 1:26, sep="-")
paste(LETTERS, 1:26, sep="-", collapse = "")
# Lets print out strings
my_string = "The year was 1979."
print(my_string)
noquote(my_string)
cat(my_string, "GRRRRRR")
# Substituting strings
chartr("a", "A", my_string)
gsub("a", "A", my_string)
# You can also check out regular expressions
?gsub
# replace digit with '_'
sub("\\d", "_", my_string)
gsub("\\d", "_", my_string)
sub("\\D", "_", my_string)
gsub("\\D", "_", my_string)
gsub("[[:digit:]]", "@", my_string)
# special characters
s = "$ It can be annoying, when people @@ put in many special characters!"
s
gsub("[[:punct:]]", "", s)
# Other regular expressions for character substitutions
# http://www.pnotepad.org/docs/search/regular_expressions/
# or look at
# **************************************
# http://gastonsanchez.com/Handling_and_Processing_Strings_in_R.pdf
# **************************************
# Taking portion of a string
s = "This is a boring string"
s
substr(s, 1, 8)
substr(s, nchar(s)-10, nchar(s))
# splitting strings
sentence = c("R is a collaborative project with many contributors")
str_split(sentence, " ")
tels = c("510-548-2238", "707-231-2440", "650-752-1300", "510-548-2239", "510-548-2403", "510-548-1111")
str_split(tels, "-")
lapply(str_split(tels, "-"), function(x) x[1])
unlist(lapply(str_split(tels, "-"), function(x) x[1]))
table(unlist(lapply(str_split(tels, "-"), function(x) x[1])))
df = data.frame(phone=tels)
df
df$First = unlist(lapply(str_split(tels, "-"), function(x) x[1]))
df$Second = unlist(lapply(str_split(tels, "-"), function(x) x[2]))
df
# Matching characters
LETTERS
match("U", LETTERS)
match("A", LETTERS)
match(650, df[,2])
df[match(650, df[,2]),]
## load tweets into R
load("data/RDataMining-Tweets-20160203.rdata")
library(tm)
library(twitteR)
tweets.df <- twListToDF(tweets)
head(tweets.df$text, 4)
# load the sentences to a corpus of documents
myCorpus <- Corpus(VectorSource(tweets.df$text))
myCorpus[[1]]$content
# convert to lower case
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
myCorpus[[1]]$content
# remove URLs
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeURL))
myCorpus[[1]]$content
# remove anything other than English letters or space
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeNumPunct))
myCorpus[[1]]$content
# Remove stopwords
myCorpus <- tm_map(myCorpus, removeWords, stopwords('english'))
myCorpus[[1]]$content
# remove extra whitespace
myCorpus <- tm_map(myCorpus, stripWhitespace)
myCorpus[[1]]$content
# keep a copy of corpus to use later as a dictionary for stem completion
# stem words
myCorpus <- tm_map(myCorpus, stemDocument)
myCorpus[[4]]$content
# stem words
myCorpus <- tm_map(myCorpus, stemDocument)
# Today we look at string operations in R.
# You will need to load the library stringr.
# In TI data preprocessing, we often want to,
#     Abbreviate strings (such as reducing the length of survey questions)
#     Stem strings - eg. we may have multiple job types which are close (System Admin & system Administrator) and we want to make them the same
#     Find or subset rows containing a certain string
#     Giving names to oridinal categories
# http://gastonsanchez.com/Handling_and_Processing_Strings_in_R.pdf
# http://www2.rdatamining.com/uploads/5/7/1/3/57136767/rdatamining-slides-text-mining.pdf
# This removes anything from a previous session
rm(list=ls())
# Lets check current time
Sys.time()
# First we set our working directory
# Please change this to your working directory where the data is
setwd("C:/Users/IBM_ADMIN/Box Sync/DScTraining/_3108_meeting")
getwd()
# Lets load our data set, and take a quick look at it
setwd("/Users/dhanley2/Documents/ml_training_r/_003_meeting")
getwd()
# Lets load our data set, and take a quick look at it
data("USArrests")
head(USArrests)
dim(USArrests)
# We see the rows are called after states of the US, and the column is the number of arrests per crime
states = rownames(USArrests)
states
substr(states, 1, 4)
# We can also use the abbreviate function which reduces to unique characters
states2 = abbreviate(states)
states2
abbreviate(states, minlength = 5)
# Lets look at the longest state name
states[1]
nchar(states[1:10])
states[which(nchar(states) > max(nchar(states))-5)]
# Get states with the string "w"
grep("w", states, value = T)
# Notice we miss Washington
grep("[wW]", states, value = T)  # ... this searches for both lower and upper case
# Alternate approach is make states lower case
tolower(states)
grep("w", tolower(states), value = T)
# We can also do uppercase
toupper(states)
# Or we can check in the help
?grep
grep("w", states, ignore.case = T, value = T)
# Now lets say we want to look for specific letters in a strings
library(stringr)
states[1]
str_count(states[1], "a")
states[7]
str_count(states[7], "a")
str_count(states, "a")
# Now we will look at String Manipulations
pi
PI = paste("Proj/Act", pi)
PI
IloveR = paste("I", "love", "R", sep = "-")
IloveR
paste("Column", 1:5, sep = ".")
# Create dynamic columns
letters
LETTERS
paste(LETTERS, 1:26, sep="-")
paste(LETTERS, 1:26, sep="-", collapse = "")
# Lets print out strings
my_string = "The year was 1979."
print(my_string)
noquote(my_string)
cat(my_string, "GRRRRRR")
# Substituting strings
chartr("a", "A", my_string)
gsub("a", "A", my_string)
# You can also check out regular expressions
?gsub
# replace digit with '_'
sub("\\d", "_", my_string)
gsub("\\d", "_", my_string)
sub("\\D", "_", my_string)
gsub("\\D", "_", my_string)
gsub("[[:digit:]]", "@", my_string)
# special characters
s = "$ It can be annoying, when people @@ put in many special characters!"
s
gsub("[[:punct:]]", "", s)
# Other regular expressions for character substitutions
# http://www.pnotepad.org/docs/search/regular_expressions/
# or look at
# **************************************
# http://gastonsanchez.com/Handling_and_Processing_Strings_in_R.pdf
# **************************************
# Taking portion of a string
s = "This is a boring string"
s
substr(s, 1, 8)
substr(s, nchar(s)-10, nchar(s))
# splitting strings
sentence = c("R is a collaborative project with many contributors")
str_split(sentence, " ")
tels = c("510-548-2238", "707-231-2440", "650-752-1300", "510-548-2239", "510-548-2403", "510-548-1111")
str_split(tels, "-")
lapply(str_split(tels, "-"), function(x) x[1])
unlist(lapply(str_split(tels, "-"), function(x) x[1]))
table(unlist(lapply(str_split(tels, "-"), function(x) x[1])))
df = data.frame(phone=tels)
df
df$First = unlist(lapply(str_split(tels, "-"), function(x) x[1]))
df$Second = unlist(lapply(str_split(tels, "-"), function(x) x[2]))
df
# Matching characters
LETTERS
match("U", LETTERS)
match("A", LETTERS)
match(650, df[,2])
df[match(650, df[,2]),]
## load tweets into R
load("data/RDataMining-Tweets-20160203.rdata")
library(tm)
library(twitteR)
tweets.df <- twListToDF(tweets)
head(tweets.df$text, 4)
# load the sentences to a corpus of documents
myCorpus <- Corpus(VectorSource(tweets.df$text))
myCorpus[[1]]$content
# convert to lower case
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
myCorpus[[1]]$content
# remove URLs
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeURL))
myCorpus[[1]]$content
# remove anything other than English letters or space
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeNumPunct))
myCorpus[[1]]$content
# Remove stopwords
myCorpus <- tm_map(myCorpus, removeWords, stopwords('english'))
myCorpus[[1]]$content
# remove extra whitespace
myCorpus <- tm_map(myCorpus, stripWhitespace)
myCorpus[[1]]$content
# keep a copy of corpus to use later as a dictionary for stem completion
# stem words
myCorpus <- tm_map(myCorpus, stemDocument)
myCorpus[[4]]$content
# stem words
myCorpus <- tm_map(myCorpus, stemDocument)
# Extract the content and compare before and after
unlist(lapply(myCorpus, content))[1:5]
lapply(myCorpus, content)
myCorpus
myCorpus[[1]]
content(myCorpus[[1]])
content(myCorpus[[0]])
content(myCorpus[[2]])
content(myCorpus[[3]])
content(myCorpus[[4]])
for (i in 1:5) content(myCorpus[[i]])
for (i in 1:5) print(content(myCorpus[[i]]))
unlist(lapply(myCorpus[[1:5]], content))[1:5]
unlist(lapply(myCorpus[[1:5]], content))
unlist(lapply(myCorpus[1:5], content))
for (i in 1:5) print(content(myCorpus[[i]]))
for (i in 1:400) print(type(myCorpus[[i]]))
for (i in 1:400) print(class(myCorpus[[i]]))
for (i in 1:447) print(class(myCorpus[[i]]))
unlist(apply(myCorpus, content))
unlist(lapply(myCorpus, content))
unlist(lapply(myCorpus, content))
for (i in 1:5) print(content(myCorpus[[i]]))
for (i in 1:447) print(class(myCorpus[[i]]))
