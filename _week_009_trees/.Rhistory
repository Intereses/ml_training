install.packages("LightGBM")
library(devtools)
install.packages("devtools")
options(devtools.install.args = "--no-multiarch")
install_github("Microsoft/LightGBM", subdir = "R-package")
install_github("Microsoft/LightGBM", subdir = "R-package")
options(devtools.install.args = "--no-multiarch")
install_github("Microsoft/LightGBM", subdir = "R-package")
# Create some variables
6+6
a = 3
b = 4
a
b
# Lets add them
a + b
# lets try a character vector
char = c("The brown cow", "said", "moo")
char[3]
# Boolean
bool = c(TRUE, TRUE, FALSE, T, F, T, F, F)
bool
# now lets create variables
v1 = c(1, 5, 9, 9, 5, 5, 5, 5)
v2 = c(a, b, 7, 5, 5, 5, 5, 5)
v1 * v2
# now lets put them in a matrix
m1 = matrix(c(v1, v2), nrow = length(v1))
m1
# rename the colums
colnames(m1) = c("First", "Second")
m1
rownames(m1) = 1:nrow(m1)
m1
# lets get a description of our matrix
summary(m1)
?summary
# lets get a compact description of our matrix
str(m1)
?str
# If we want to add a different type of variable to a matrix we need to use a data frame
df1 = data.frame(col1 = v1, col2 = v2, col4 = bool)
df1
str(df1)
# Today we look at Linear Regression in both R and TI.
# Please use the below book as a reference - its an  excellent resource for getting started in many ML tasks.
#    https://ibm.box.com/s/9qa5l146in39u2lmfb1ln29lfif8yr6v
# This removes anything from a previous session
rm(list=ls())
# Lets check current time
Sys.time()
getwd()
# Libraries
library(Metrics)
library(caret)
library(ISLR)
# Before we start, what is a logistic function ??
install.packages("ISLR")
#install.packages("ISLR")
library(ISLR)
# Before we start, what is a logistic function ??
e = 2.71828   # natural log
log(0.5)
e^(log(0.5))
log(1)
log(10)
log(100000000000000000)
# Let look at how it behaves
sequence = 0:100
plot(sequence, log(sequence), type="l", col="red")
log(0)
# Function for logistics regression
myfunc = function(x) (e^x/(1+e^x))
sequence = (-100:100)/10
sequence
plot(sequence, myfunc(sequence), type="l", col="red")
# shift the function to a different intercept
myfunc = function(x) (e^(x-3)/(1+e^(x-3)))
plot(sequence, myfunc(sequence), type="l", col="red")
# flip the function
myfunc = function(x) (e^(-x)/(1+e^(-x)))
plot(sequence, myfunc(sequence), type="l", col="red")
# We are going to use the US cereals data set
data(Caravan)
dim(Caravan)
str(Caravan)
plot(sequence, myfunc(sequence), type="l", col="red")
# This removes anything from a previous session
rm(list=ls())
# Lets check current time
Sys.time()
# First we set our working directory
setwd("/Users/dhanley2/Documents/ml_training_r/_009_meeting")
getwd()
# Libraries
library(tree)
library(MASS)
library(Metrics)
library(caret)
set.seed(1)
# Lets look at Hitters data
data("Boston")
str(Boston)
# Median house value is the target
x=Boston
x$medv = NULL
y=Boston$medv
x=Boston
x$medv = NULL
y=Boston$medv
# Lets split our data into two folds and predict
set.seed(1)
ind = createFolds(y, 2, list=F)
X_train = x[ind==1, ]
X_test  = x[ind!=1, ]
y_train = y[ind==1 ]
y_test  = y[ind!=1 ]
# Now lets model
tree.boston = tree(y_train ~ . , X_train)
library(tree)
install.packages(tree)
install.packages("tree")
library(tree)
# Now lets model
tree.boston = tree(y_train ~ . , X_train)
summary (tree.boston )
plot(tree.boston)
text(tree.boston ,pretty =0)
?tree
?tree
cv.boston =cv.tree(tree.boston )
plot(cv.boston$size ,cv.boston$dev ,type="b")
# Now we prune the tree
prune.boston =prune.tree(tree.boston ,best =5)
plot(prune.boston )
text(prune.boston ,pretty =0)
# Let us predict on the tree
ypred.tree = predict(tree.boston , newdata = X_test)
plot(y_test, ypred.tree, col="blue", pch=19)
abline (0,1)
rmse(y_test, ypred.tree)
# 5.059668
plot(prune.boston )
text(prune.boston )
text(prune.boston ,pretty =0)
text(prune.boston ,pretty =1)
# Let us predict on the tree
ypred.tree = predict(tree.boston , newdata = X_test)
plot(y_test, ypred.tree, col="blue", pch=19)
abline (0,1)
rmse(y_test, ypred.tree)
# 5.059668
# Lets compare to a linear model type approach
lm.mod = lm(y_train~., data=data.frame(X_train))
ypred.lm = predict(lm.mod, data.frame(X_test))
points(ypred.lm, col="red", pch=19)
rmse(y_test, ypred.lm)
# 4.679406
# Lets check if we combine the two
# These two models work a lot differently, so a combination of the two can often do better than one on its own.
rmse(y_test, (ypred.lm+ypred.tree)/2)
# This removes anything from a previous session
rm(list=ls())
# Lets check current time
Sys.time()
# First we set our working directory
# Please change this to your working directory where the data is
setwd("C:/Users/IBM_ADMIN/Box Sync/DScTraining/_0911_meeting")
getwd()
# Libraries
library(tree)
library(MASS)
library(Metrics)
library(caret)
library (randomForest)
set.seed(1)
# Lets look at Hitters data
data("Boston")
str(Boston)
# Lets create our training and test data
# Median house value is the target
x=Boston
x$medv = NULL
y=Boston$medv
# Lets split our data into two folds and predict
set.seed(1)
ind = createFolds(y, 2, list=F)
X_train = x[ind==1, ]
X_test  = x[ind!=1, ]
y_train = y[ind==1 ]
y_test  = y[ind!=1 ]
# Now lets model
tree.boston = tree(y_train ~ . , X_train)
summary (tree.boston )
plot(tree.boston)
text(tree.boston ,pretty =0)
?tree
cv.boston =cv.tree(tree.boston )
plot(cv.boston$size ,cv.boston$dev ,type="b")
# Now we prune the tree
prune.boston =prune.tree(tree.boston ,best =7)
plot(prune.boston )
text(prune.boston ,pretty =0)
# Let us predict on the tree
ypred.tree = predict(tree.boston , newdata = X_test)
plot(y_test, ypred.tree, col="blue", pch=19)
abline (0,1)
rmse(y_test, ypred.tree)
# 5.059668
# Lets compare to a linear model type approach
lm.mod = lm(y_train~., data=data.frame(X_train))
ypred.lm = predict(lm.mod, data.frame(X_test))
points(ypred.lm, col="red", pch=19)
rmse(y_test, ypred.lm)
# 4.679406
# Lets check if we combine the two
# These two models work a lot differently, so a combination of the two can often do better than one on its own.
rmse(y_test, (ypred.lm+ypred.tree)/2)
# [1] 4.22
# Random Forest
?randomForest
# This removes anything from a previous session
rm(list=ls())
# Lets check current time
Sys.time()
# First we set our working directory
# Please change this to your working directory where the data is
setwd("C:/Users/IBM_ADMIN/Box Sync/DScTraining/_0911_meeting")
getwd()
# Libraries
library(tree)
library(MASS)
library(Metrics)
library(caret)
install.packages("randomForest")
library (randomForest)
set.seed(1)
# Lets look at Hitters data
data("Boston")
str(Boston)
# Lets create our training and test data
# Median house value is the target
x=Boston
x$medv = NULL
y=Boston$medv
# Lets split our data into two folds and predict
set.seed(1)
ind = createFolds(y, 2, list=F)
X_train = x[ind==1, ]
X_test  = x[ind!=1, ]
y_train = y[ind==1 ]
y_test  = y[ind!=1 ]
# Now lets model
tree.boston = tree(y_train ~ . , X_train)
summary (tree.boston )
plot(tree.boston)
text(tree.boston ,pretty =0)
?tree
cv.boston =cv.tree(tree.boston )
plot(cv.boston$size ,cv.boston$dev ,type="b")
# Now we prune the tree
prune.boston =prune.tree(tree.boston ,best =7)
plot(prune.boston )
text(prune.boston ,pretty =0)
# Let us predict on the tree
ypred.tree = predict(tree.boston , newdata = X_test)
plot(y_test, ypred.tree, col="blue", pch=19)
abline (0,1)
rmse(y_test, ypred.tree)
# 5.059668
# Lets compare to a linear model type approach
lm.mod = lm(y_train~., data=data.frame(X_train))
ypred.lm = predict(lm.mod, data.frame(X_test))
points(ypred.lm, col="red", pch=19)
rmse(y_test, ypred.lm)
# 4.679406
# Lets check if we combine the two
# These two models work a lot differently, so a combination of the two can often do better than one on its own.
rmse(y_test, (ypred.lm+ypred.tree)/2)
# [1] 4.22
# Random Forest
?randomForest
set.seed (1)
rf.mod = randomForest(y_train~.,data=X_train, mtry=6, importance =TRUE) # ranger
rf.mod = randomForest(as.factor(loan_status)~.,data=X_train, mtry=6, importance =TRUE) # ranger
ypred.rf = predict(rf.mod ,newdata = X_test)
rmse(y_test, ypred.rf)
# [1] 3.907974
# Now lets look at the importance
importance(rf.mod)
# Lets sort it
imp = importance(rf.mod)
imp[order(imp[,1], decreasing = T),]
# Now let's tune to see if we can do better
# First we tune over a different number of variables per tree
set.seed(7)
mtry = 2:13
error = rep(0, length(2:13))
for(m in 1:length(mtry)){
rf.mod = randomForest(y_train~.,data=X_train, mtry=mtry[m], ntree = 500)
ypred.rf = predict(rf.mod ,newdata = X_test)
error[m] = rmse(y_test, ypred.rf)
plot(mtry, error, pch = 19, col="blue", ylim = c(3.5, 4.5), main = "Random Forest : Error for Number of Random Features per Tree")
}
# Now we keep number of variables constant and try changing the number of trees
m = 5
trees = seq(100, 3000, 100)
error = rep(0, length(trees))
for(t in 1:length(error)){
rf.mod = randomForest(y_train~.,data=X_train, mtry=m, ntree = trees[t])
ypred.rf = predict(rf.mod ,newdata = X_test)
error[t] = rmse(y_test, ypred.rf)
plot(trees, error, pch = 19, col="blue", ylim = c(3.5, 4.5), main = "Random Forest : Error for Different Number of Trees")
}
rmse(y_test, ypred.rf)
# [1] 3.82486
# Finally, lets check blending the randomforest and linear model.
rmse(y_test, (ypred.rf+ypred.lm)/2)
# [1] 3.951797
# Lets try a weighted blend
rmse(y_test, ((ypred.rf*0.8)+(ypred.lm*0.2)))
# [1] 3.793317
